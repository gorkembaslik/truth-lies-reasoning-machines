{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 02 - API Setup and Testing\n",
				"\n",
				"This notebook tests the connection to Gemini and GitHub Models APIs.\n",
				"\n",
				"## Prerequisites\n",
				"\n",
				"Before running this notebook, make sure you have:\n",
				"1. Created a `.env` file with your API keys\n",
				"2. Installed all requirements (`pip install -r requirements.txt`)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Setup\n",
				"import sys\n",
				"sys.path.insert(0, '..')\n",
				"\n",
				"import os\n",
				"from pathlib import Path\n",
				"from dotenv import load_dotenv\n",
				"\n",
				"# Load environment variables\n",
				"env_path = Path('../.env')\n",
				"if env_path.exists():\n",
				"    load_dotenv(env_path)\n",
				"    print(\"✓ Loaded .env file\")\n",
				"else:\n",
				"    print(\"✗ .env file not found! \")\n",
				"    print(\"  Please copy .env.example to .env and add your API keys\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 1. Test Google Gemini API\n",
				"\n",
				"We'll use the `google-generativeai` library which is simpler than the full Vertex AI SDK."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Check if API key is set\n",
				"google_api_key = os.getenv('GOOGLE_API_KEY')\n",
				"\n",
				"if google_api_key:\n",
				"    print(f\"✓ GOOGLE_API_KEY is set (starts with:  {google_api_key[:10]}...)\")\n",
				"else:\n",
				"    print(\"✗ GOOGLE_API_KEY is not set!\")\n",
				"    print(\"  Get your API key from: https://makersuite.google.com/app/apikey\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Test Gemini API\n",
				"if google_api_key:\n",
				"    from src.models import GeminiClient\n",
				"    \n",
				"    try:\n",
				"        # Initialize client\n",
				"        gemini = GeminiClient(model_name=\"gemini-1.5-flash\", temperature=0.0)\n",
				"        print(f\"✓ Initialized:  {gemini}\")\n",
				"        \n",
				"        # Test simple query\n",
				"        response = gemini.generate(\"What is 2 + 2?  Answer with just the number.\")\n",
				"        print(f\"\\n✓ Test query successful! \")\n",
				"        print(f\"  Response: {response.text.strip()}\")\n",
				"        print(f\"  Model:  {response.model}\")\n",
				"        print(f\"  Latency: {response.latency_ms:.0f}ms\")\n",
				"        if response.total_tokens:\n",
				"            print(f\"  Tokens used: {response.total_tokens}\")\n",
				"        \n",
				"    except Exception as e:\n",
				"        print(f\"✗ Error: {e}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Test with a more complex reasoning question\n",
				"if google_api_key and 'gemini' in dir():\n",
				"    test_question = \"\"\"\n",
				"    Question: If all roses are flowers and some flowers fade quickly, \n",
				"    can we conclude that some roses fade quickly?\n",
				"    \n",
				"    Think step by step and provide your answer.\n",
				"    \"\"\"\n",
				"    \n",
				"    response = gemini.generate(test_question, max_tokens=500)\n",
				"    print(\"Reasoning Test: \")\n",
				"    print(\"=\" * 60)\n",
				"    print(response.text)\n",
				"    print(\"=\" * 60)\n",
				"    print(f\"Latency: {response.latency_ms:.0f}ms\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 2. Test GitHub Models API (Backup)\n",
				"\n",
				"GitHub Models provides free access to various LLMs through your GitHub token."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Check if GitHub token is set\n",
				"github_token = os.getenv('GITHUB_TOKEN')\n",
				"\n",
				"if github_token:\n",
				"    print(f\"✓ GITHUB_TOKEN is set (starts with: {github_token[:10]}...)\")\n",
				"else:\n",
				"    print(\"✗ GITHUB_TOKEN is not set!\")\n",
				"    print(\"  Create a token at: https://github.com/settings/tokens\")\n",
				"    print(\"  Note: This is your backup option if Gemini credits run out\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Test GitHub Models API\n",
				"if github_token:\n",
				"    from src.models import GitHubModelsClient\n",
				"    \n",
				"    try:\n",
				"        # Initialize client\n",
				"        gh_models = GitHubModelsClient(model_name=\"gpt-4o-mini\", temperature=0.0)\n",
				"        print(f\"✓ Initialized: {gh_models}\")\n",
				"        \n",
				"        # Test simple query\n",
				"        response = gh_models.generate(\"What is 2 + 2?  Answer with just the number.\")\n",
				"        print(f\"\\n✓ Test query successful!\")\n",
				"        print(f\"  Response: {response.text.strip()}\")\n",
				"        print(f\"  Model: {response.model}\")\n",
				"        print(f\"  Latency: {response.latency_ms:.0f}ms\")\n",
				"        if response.total_tokens:\n",
				"            print(f\"  Tokens used: {response.total_tokens}\")\n",
				"        \n",
				"    except Exception as e:\n",
				"        print(f\"✗ Error:  {e}\")\n",
				"        print(\"  GitHub Models may require specific token permissions. \")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 3. Test Prompt Templates"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from src.models import get_prompt, list_prompts, PromptType\n",
				"\n",
				"# List all available prompts\n",
				"print(\"Available Prompt Templates:\")\n",
				"print(\"=\" * 40)\n",
				"\n",
				"for prompt_type in PromptType:\n",
				"    prompts = list_prompts(prompt_type)\n",
				"    print(f\"\\n{prompt_type.value.upper()}:\")\n",
				"    for name in prompts:\n",
				"        print(f\"  - {name}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Test a prompt template\n",
				"cot_prompt = get_prompt(\"cot_qa\")\n",
				"print(f\"Prompt:  {cot_prompt.name}\")\n",
				"print(f\"Type: {cot_prompt.prompt_type.value}\")\n",
				"print(f\"Description: {cot_prompt.description}\")\n",
				"print(f\"\\nTemplate:\\n{cot_prompt.template}\")\n",
				"print(f\"\\nFormatted Example: \")\n",
				"print(cot_prompt.format(question=\"What is the capital of France?\"))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Test chain-of-thought prompting with Gemini\n",
				"if 'gemini' in dir():\n",
				"    # A question that benefits from step-by-step reasoning\n",
				"    question = \"If a train travels at 60 mph for 2.5 hours, how far does it go?\"\n",
				"    \n",
				"    # Baseline prompt\n",
				"    baseline = get_prompt(\"baseline_qa\")\n",
				"    baseline_response = gemini.generate(baseline.format(question=question))\n",
				"    \n",
				"    # Chain-of-thought prompt\n",
				"    cot = get_prompt(\"cot_qa\")\n",
				"    cot_response = gemini.generate(cot.format(question=question))\n",
				"    \n",
				"    print(\"Baseline Response:\")\n",
				"    print(baseline_response.text)\n",
				"    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
				"    print(\"Chain-of-Thought Response:\")\n",
				"    print(cot_response.text)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 4. Quick Integration Test\n",
				"\n",
				"Let's run a quick end-to-end test with TruthfulQA."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from src.data import TruthfulQADataset\n",
				"from src.evaluation import MetricsCalculator, truthfulness_score\n",
				"from pathlib import Path\n",
				"\n",
				"# Load a small sample\n",
				"truthfulqa_path = Path('../data/raw/TruthfulQA.csv')\n",
				"\n",
				"if truthfulqa_path.exists() and 'gemini' in dir():\n",
				"    dataset = TruthfulQADataset(str(truthfulqa_path))\n",
				"    samples = dataset.sample(3, seed=42)\n",
				"    \n",
				"    calculator = MetricsCalculator()\n",
				"    \n",
				"    print(\"Running integration test...\\n\")\n",
				"    \n",
				"    for example in samples:\n",
				"        # Get model response\n",
				"        prompt = f\"Question: {example.question}\\n\\nProvide a brief, factual answer:\"\n",
				"        response = gemini.generate(prompt, max_tokens=100)\n",
				"        \n",
				"        # Calculate metrics\n",
				"        result = calculator.add_result(\n",
				"            example_id=example.id,\n",
				"            prediction=response.text,\n",
				"            ground_truth=example.correct_answer,\n",
				"            incorrect_answers=example.incorrect_answers\n",
				"        )\n",
				"        \n",
				"        print(f\"Q: {example.question}\")\n",
				"        print(f\"Model: {response.text.strip()[:100]}...\")\n",
				"        print(f\"Correct: {example.correct_answer}\")\n",
				"        print(f\"F1 Score: {result['f1_correct']:.2f}\")\n",
				"        print(\"-\" * 60)\n",
				"    \n",
				"    # Aggregate metrics\n",
				"    print(\"\\nAggregate Metrics: \")\n",
				"    print(calculator.get_aggregate_metrics())\n",
				"else:\n",
				"    print(\"Please ensure TruthfulQA dataset is downloaded and Gemini API is configured. \")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 5. Summary\n",
				"\n",
				"### API Status"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"print(\"API Setup Summary:\")\n",
				"print(\"=\" * 40)\n",
				"print(f\"Google Gemini:  {'✓ Ready' if google_api_key else '✗ Not configured'}\")\n",
				"print(f\"GitHub Models:  {'✓ Ready' if github_token else '✗ Not configured'}\")\n",
				"print(\"\\nRecommendation:\")\n",
				"if google_api_key:\n",
				"    print(\"  Use Gemini 1.5 Flash for development and bulk experiments\")\n",
				"    print(\"  Use Gemini 1.5 Pro for final evaluation runs\")\n",
				"if github_token:\n",
				"    print(\"  GitHub Models available as backup if Gemini credits run low\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"print(\"\\nNext Steps:\")\n",
				"print(\"1. If APIs are working, proceed to 03_baseline_experiments.ipynb\")\n",
				"print(\"2. Run baseline experiments on TruthfulQA and HotpotQA\")\n",
				"print(\"3. Then move to perturbation experiments\")"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"name": "python",
			"version": "3.10.0"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}