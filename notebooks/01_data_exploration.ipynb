{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 01 - Data Exploration\n",
				"\n",
				"This notebook explores the TruthfulQA and HotpotQA datasets to understand their structure and content.\n",
				"\n",
				"## Setup"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Add project root to path\n",
				"import sys\n",
				"sys.path.insert(0, '..')\n",
				"\n",
				"# Import libraries\n",
				"import pandas as pd\n",
				"import matplotlib.pyplot as plt\n",
				"import seaborn as sns\n",
				"from pathlib import Path\n",
				"\n",
				"# Import our modules\n",
				"from src.data import TruthfulQADataset, HotpotQADataset, DistortionType\n",
				"\n",
				"# Set display options\n",
				"pd.set_option('display.max_colwidth', 100)\n",
				"plt.style.use('seaborn-v0_8-whitegrid')\n",
				"\n",
				"print(\"Setup complete!\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 1. TruthfulQA Dataset\n",
				"\n",
				"TruthfulQA measures whether language models mimic human falsehoods. It contains 817 questions designed to induce false answers."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Load TruthfulQA\n",
				"truthfulqa_path = Path('../data/raw/TruthfulQA.csv')\n",
				"\n",
				"if truthfulqa_path.exists():\n",
				"    truthfulqa = TruthfulQADataset(str(truthfulqa_path))\n",
				"    print(f\"Loaded TruthfulQA:  {len(truthfulqa)} questions\")\n",
				"else:\n",
				"    print(f\"Dataset not found at {truthfulqa_path}\")\n",
				"    print(\"Please download it using: \")\n",
				"    print(\"wget -O data/raw/TruthfulQA.csv https://raw.githubusercontent.com/sylinrl/TruthfulQA/main/TruthfulQA.csv\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Explore dataset statistics\n",
				"if 'truthfulqa' in dir():\n",
				"    stats = truthfulqa.get_statistics()\n",
				"    print(f\"Total questions: {stats['total_examples']}\")\n",
				"    print(f\"Number of categories: {stats['num_categories']}\")\n",
				"    print(f\"\\nCategories: {truthfulqa.get_categories()}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# View category distribution\n",
				"if 'truthfulqa' in dir():\n",
				"    category_summary = truthfulqa.get_categories_summary()\n",
				"    print(\"Category Distribution:\")\n",
				"    print(category_summary.head(15))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Plot category distribution\n",
				"if 'truthfulqa' in dir():\n",
				"    fig, ax = plt.subplots(figsize=(12, 8))\n",
				"    \n",
				"    summary = truthfulqa.get_categories_summary().head(15)\n",
				"    colors = sns.color_palette('husl', len(summary))\n",
				"    \n",
				"    bars = ax.barh(summary['Category'], summary['Count'], color=colors)\n",
				"    ax.set_xlabel('Number of Questions')\n",
				"    ax.set_title('TruthfulQA:  Top 15 Categories by Question Count')\n",
				"    \n",
				"    for bar, count in zip(bars, summary['Count']):\n",
				"        ax.text(count + 1, bar.get_y() + bar.get_height()/2, str(count), va='center')\n",
				"    \n",
				"    plt.tight_layout()\n",
				"    plt.show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Look at some example questions\n",
				"if 'truthfulqa' in dir():\n",
				"    print(\"Sample Questions from TruthfulQA:\\n\")\n",
				"    print(\"=\" * 80)\n",
				"    \n",
				"    for example in truthfulqa.sample(5, seed=42):\n",
				"        print(f\"Category: {example.category}\")\n",
				"        print(f\"Question: {example.question}\")\n",
				"        print(f\"Correct Answer:  {example.correct_answer}\")\n",
				"        if example.incorrect_answers:\n",
				"            print(f\"Common Wrong Answer: {example.incorrect_answers[0]}\")\n",
				"        print(\"-\" * 80)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Get adversarial pairs (correct vs incorrect answers)\n",
				"if 'truthfulqa' in dir():\n",
				"    pairs = truthfulqa.get_adversarial_pairs()\n",
				"    print(f\"Found {len(pairs)} adversarial pairs\\n\")\n",
				"    \n",
				"    print(\"Example Adversarial Pairs:\")\n",
				"    for pair in pairs[:3]:\n",
				"        print(f\"Q:  {pair['question']}\")\n",
				"        print(f\"✓ Correct:  {pair['correct_answer']}\")\n",
				"        print(f\"✗ Incorrect: {pair['incorrect_answer']}\")\n",
				"        print()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 2. HotpotQA Dataset\n",
				"\n",
				"HotpotQA is a dataset for multi-hop question answering that requires reasoning over multiple documents."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Load HotpotQA\n",
				"hotpotqa_path = Path('../data/raw/hotpot_dev_distractor_v1.json')\n",
				"\n",
				"if hotpotqa_path.exists():\n",
				"    # Load only first 500 examples for exploration\n",
				"    hotpotqa = HotpotQADataset(str(hotpotqa_path), max_examples=500)\n",
				"    print(f\"Loaded HotpotQA: {len(hotpotqa)} questions\")\n",
				"else:\n",
				"    print(f\"Dataset not found at {hotpotqa_path}\")\n",
				"    print(\"Please download it using:\")\n",
				"    print(\"wget -O data/raw/hotpot_dev_distractor_v1.json http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_dev_distractor_v1.json\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Explore HotpotQA statistics\n",
				"if 'hotpotqa' in dir():\n",
				"    stats = hotpotqa.get_statistics()\n",
				"    print(\"HotpotQA Statistics:\")\n",
				"    print(f\"Total questions: {stats['total_examples']}\")\n",
				"    print(f\"\\nQuestion types: {stats.get('question_types', {})}\")\n",
				"    print(f\"Difficulty levels: {stats.get('difficulty_levels', {})}\")\n",
				"    print(f\"\\nAverage context length: {stats.get('avg_context_length', 0):.0f} characters\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Plot question type and difficulty distribution\n",
				"if 'hotpotqa' in dir():\n",
				"    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
				"    \n",
				"    stats = hotpotqa.get_statistics()\n",
				"    \n",
				"    # Question types\n",
				"    types = stats.get('question_types', {})\n",
				"    if types:\n",
				"        axes[0].pie(types.values(), labels=types.keys(), autopct='%1.1f%%', colors=['#3498db', '#e74c3c'])\n",
				"        axes[0].set_title('Question Types')\n",
				"    \n",
				"    # Difficulty levels\n",
				"    levels = stats.get('difficulty_levels', {})\n",
				"    if levels:\n",
				"        colors = ['#2ecc71', '#f1c40f', '#e74c3c']\n",
				"        axes[1].pie(levels.values(), labels=levels.keys(), autopct='%1.1f%%', colors=colors)\n",
				"        axes[1].set_title('Difficulty Levels')\n",
				"    \n",
				"    plt.tight_layout()\n",
				"    plt.show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Look at a sample HotpotQA question\n",
				"if 'hotpotqa' in dir():\n",
				"    example = hotpotqa[0]\n",
				"    \n",
				"    print(\"Sample HotpotQA Question:\")\n",
				"    print(\"=\" * 80)\n",
				"    print(f\"ID: {example.id}\")\n",
				"    print(f\"Type: {example.category}\")\n",
				"    print(f\"Difficulty: {example. difficulty}\")\n",
				"    print(f\"\\nQuestion: {example.question}\")\n",
				"    print(f\"\\nAnswer: {example.correct_answer}\")\n",
				"    print(f\"\\nSupporting Facts:\")\n",
				"    for i, fact in enumerate(example.supporting_facts or [], 1):\n",
				"        print(f\"  {i}.{fact}\")\n",
				"    print(f\"\\nContext (first 500 chars):\")\n",
				"    print(example.context[:500] if example.context else \"N/A\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Get multi-hop examples\n",
				"if 'hotpotqa' in dir():\n",
				"    multi_hop = hotpotqa.get_multi_hop_examples(min_hops=2)\n",
				"    print(f\"Found {len(multi_hop)} multi-hop questions (requiring 2+ sources)\")\n",
				"    \n",
				"    print(\"\\nExample Multi-Hop Question:\")\n",
				"    if multi_hop:\n",
				"        ex = multi_hop[0]\n",
				"        print(f\"Q: {ex.question}\")\n",
				"        print(f\"A: {ex.correct_answer}\")\n",
				"        print(f\"Supporting facts from {len(set(t for t, _ in ex.metadata.get('raw_supporting_facts', [])))} different sources\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 3. Summary\n",
				"\n",
				"### Dataset Comparison\n",
				"\n",
				"| Feature | TruthfulQA | HotpotQA |\n",
				"|---------|------------|----------|\n",
				"| Purpose | Test factual accuracy | Test multi-hop reasoning |\n",
				"| Size | 817 questions | 7,405 dev questions |\n",
				"| Context | No context | Multi-paragraph context |\n",
				"| Categories | 38 categories | 2 types (bridge, comparison) |\n",
				"| Difficulty | N/A | Easy, Medium, Hard |\n",
				"| Use in project | Misconception testing | Perturbation experiments |"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"print(\"Data exploration complete! \")\n",
				"print(\"\\nNext steps:\")\n",
				"print(\"1. Run notebook 02_api_setup_test.ipynb to test API connections\")\n",
				"print(\"2. Then proceed to baseline experiments\")"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"name": "python",
			"version": "3.10.0"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}