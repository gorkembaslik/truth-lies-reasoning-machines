{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 03 - Baseline Experiments\n",
				"\n",
				"This notebook establishes baseline performance on TruthfulQA and HotpotQA before applying perturbations.\n",
				"\n",
				"## Objectives\n",
				"1. Evaluate model performance on unmodified datasets\n",
				"2. Compare different prompting strategies (baseline vs chain-of-thought)\n",
				"3. Establish reference metrics for later comparison with perturbed experiments"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Setup\n",
				"import sys\n",
				"sys.path.insert(0, '..')\n",
				"\n",
				"import os\n",
				"import json\n",
				"import pandas as pd\n",
				"import matplotlib.pyplot as plt\n",
				"from pathlib import Path\n",
				"from datetime import datetime\n",
				"from tqdm import tqdm\n",
				"from dotenv import load_dotenv\n",
				"\n",
				"# Load environment\n",
				"load_dotenv(Path('../.env'))\n",
				"\n",
				"# Import project modules\n",
				"from src.data import TruthfulQADataset, HotpotQADataset\n",
				"from src.models import GeminiClient, get_prompt, PromptType\n",
				"from src.evaluation import MetricsCalculator, truthfulness_score, f1_score\n",
				"\n",
				"# Settings\n",
				"RANDOM_SEED = 42\n",
				"RESULTS_DIR = Path('../data/results')\n",
				"RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
				"\n",
				"print(\"Setup complete!\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Initialize model\n",
				"# Using gemini-2.0-flash-lite for cost efficiency during experiments\n",
				"llm = GeminiClient(model_name=\"gemini-2.0-flash-lite-001\", temperature=0.0)\n",
				"print(f\"Model:  {llm}\")\n",
				"print(f\"API Type: {llm.get_api_type()}\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 1. TruthfulQA Baseline Experiments\n",
				"\n",
				"We'll test the model on TruthfulQA to establish baseline truthfulness metrics."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Load TruthfulQA\n",
				"truthfulqa = TruthfulQADataset('../data/raw/TruthfulQA.csv')\n",
				"print(f\"Loaded TruthfulQA: {len(truthfulqa)} questions\")\n",
				"print(f\"Categories: {len(truthfulqa.get_categories())}\")\n",
				"\n",
				"# Show category distribution\n",
				"truthfulqa.get_categories_summary().head(10)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Sample for experiments (to manage API costs)\n",
				"# We'll use 100 questions for baseline, stratified across categories\n",
				"\n",
				"TRUTHFULQA_SAMPLE_SIZE = 100\n",
				"\n",
				"# Get samples from each category proportionally\n",
				"categories = truthfulqa.get_categories()\n",
				"samples_per_category = max(1, TRUTHFULQA_SAMPLE_SIZE // len(categories))\n",
				"\n",
				"truthfulqa_samples = []\n",
				"for category in categories:\n",
				"    cat_examples = truthfulqa.get_by_category(category)\n",
				"    n_samples = min(samples_per_category, len(cat_examples))\n",
				"    import random\n",
				"    random.seed(RANDOM_SEED)\n",
				"    truthfulqa_samples.extend(random.sample(cat_examples, n_samples))\n",
				"\n",
				"# Shuffle\n",
				"random.shuffle(truthfulqa_samples)\n",
				"truthfulqa_samples = truthfulqa_samples[:TRUTHFULQA_SAMPLE_SIZE]\n",
				"\n",
				"print(f\"Selected {len(truthfulqa_samples)} samples across {len(categories)} categories\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def run_truthfulqa_experiment(\n",
				"    samples:  list,\n",
				"    llm: GeminiClient,\n",
				"    prompt_template: str = \"baseline_qa\",\n",
				"    experiment_name: str = \"baseline\"\n",
				") -> tuple[MetricsCalculator, list[dict]]:\n",
				"    \"\"\"Run TruthfulQA experiment with given prompt template.\n",
				"    \n",
				"    Args:\n",
				"        samples: List of QAExample objects\n",
				"        llm: The LLM client\n",
				"        prompt_template: Name of prompt template to use\n",
				"        experiment_name: Name for this experiment\n",
				"    \n",
				"    Returns:\n",
				"        Tuple of (MetricsCalculator, raw_results_list)\n",
				"    \"\"\"\n",
				"    calculator = MetricsCalculator()\n",
				"    raw_results = []\n",
				"    prompt = get_prompt(prompt_template)\n",
				"    \n",
				"    print(f\"Running experiment: {experiment_name}\")\n",
				"    print(f\"Prompt template: {prompt_template}\")\n",
				"    print(f\"Samples: {len(samples)}\")\n",
				"    print(\"-\" * 50)\n",
				"    \n",
				"    for example in tqdm(samples, desc=experiment_name):\n",
				"        # Format prompt\n",
				"        formatted_prompt = prompt.format(question=example.question)\n",
				"        \n",
				"        # Get response\n",
				"        try:\n",
				"            response = llm.generate(formatted_prompt, max_tokens=200)\n",
				"            response_text = response.text.strip()\n",
				"            error = None\n",
				"        except Exception as e:\n",
				"            response_text = \"\"\n",
				"            error = str(e)\n",
				"        \n",
				"        # Calculate metrics\n",
				"        result = calculator.add_result(\n",
				"            example_id=example.id,\n",
				"            prediction=response_text,\n",
				"            ground_truth=example.correct_answer,\n",
				"            incorrect_answers=example.incorrect_answers,\n",
				"            metadata={\n",
				"                \"category\": example.category,\n",
				"                \"experiment\": experiment_name,\n",
				"                \"prompt_template\": prompt_template,\n",
				"            }\n",
				"        )\n",
				"        \n",
				"        # Store raw result\n",
				"        raw_results.append({\n",
				"            \"id\": example.id,\n",
				"            \"question\": example.question,\n",
				"            \"correct_answer\": example.correct_answer,\n",
				"            \"incorrect_answers\": example.incorrect_answers,\n",
				"            \"model_response\": response_text,\n",
				"            \"category\": example.category,\n",
				"            \"f1_correct\": result[\"f1_correct\"],\n",
				"            \"f1_incorrect\": result.get(\"f1_incorrect\", 0),\n",
				"            \"truthfulness\": result.get(\"truthfulness\", 0),\n",
				"            \"error\": error,\n",
				"        })\n",
				"    \n",
				"    return calculator, raw_results"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Run baseline experiment\n",
				"baseline_calculator, baseline_results = run_truthfulqa_experiment(\n",
				"    samples=truthfulqa_samples,\n",
				"    llm=llm,\n",
				"    prompt_template=\"baseline_qa\",\n",
				"    experiment_name=\"truthfulqa_baseline\"\n",
				")\n",
				"\n",
				"# Show aggregate metrics\n",
				"print(\"\\n\" + \"=\" * 50)\n",
				"print(\"BASELINE RESULTS (TruthfulQA)\")\n",
				"print(\"=\" * 50)\n",
				"baseline_metrics = baseline_calculator.get_aggregate_metrics()\n",
				"for key, value in baseline_metrics.items():\n",
				"    if isinstance(value, float):\n",
				"        print(f\"{key}: {value:.4f}\")\n",
				"    else:\n",
				"        print(f\"{key}: {value}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Run chain-of-thought experiment\n",
				"cot_calculator, cot_results = run_truthfulqa_experiment(\n",
				"    samples=truthfulqa_samples,\n",
				"    llm=llm,\n",
				"    prompt_template=\"cot_qa\",\n",
				"    experiment_name=\"truthfulqa_cot\"\n",
				")\n",
				"\n",
				"# Show aggregate metrics\n",
				"print(\"\\n\" + \"=\" * 50)\n",
				"print(\"CHAIN-OF-THOUGHT RESULTS (TruthfulQA)\")\n",
				"print(\"=\" * 50)\n",
				"cot_metrics = cot_calculator.get_aggregate_metrics()\n",
				"for key, value in cot_metrics.items():\n",
				"    if isinstance(value, float):\n",
				"        print(f\"{key}: {value:.4f}\")\n",
				"    else:\n",
				"        print(f\"{key}:  {value}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Compare baseline vs chain-of-thought\n",
				"comparison_df = pd.DataFrame({\n",
				"    \"Metric\": [\"F1 (Correct)\", \"F1 (Incorrect)\", \"Truthfulness\", \"Exact Match\"],\n",
				"    \"Baseline\": [\n",
				"        baseline_metrics.get(\"mean_f1_correct\", 0),\n",
				"        baseline_metrics.get(\"mean_f1_incorrect\", 0) if \"mean_f1_incorrect\" in str(baseline_metrics) else 0,\n",
				"        baseline_metrics.get(\"mean_truthfulness\", 0),\n",
				"        baseline_metrics.get(\"mean_exact_match_correct\", 0),\n",
				"    ],\n",
				"    \"Chain-of-Thought\": [\n",
				"        cot_metrics.get(\"mean_f1_correct\", 0),\n",
				"        cot_metrics.get(\"mean_f1_incorrect\", 0) if \"mean_f1_incorrect\" in str(cot_metrics) else 0,\n",
				"        cot_metrics.get(\"mean_truthfulness\", 0),\n",
				"        cot_metrics.get(\"mean_exact_match_correct\", 0),\n",
				"    ],\n",
				"})\n",
				"\n",
				"comparison_df[\"Difference\"] = comparison_df[\"Chain-of-Thought\"] - comparison_df[\"Baseline\"]\n",
				"print(\"\\nComparison:  Baseline vs Chain-of-Thought\")\n",
				"print(comparison_df.to_string(index=False))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Analyze results by category\n",
				"baseline_df = pd.DataFrame(baseline_results)\n",
				"\n",
				"category_performance = baseline_df.groupby('category').agg({\n",
				"    'f1_correct':  'mean',\n",
				"    'truthfulness': 'mean',\n",
				"    'id': 'count'\n",
				"}).rename(columns={'id': 'count'}).sort_values('f1_correct', ascending=False)\n",
				"\n",
				"print(\"Performance by Category (Top 10):\")\n",
				"print(category_performance.head(10).to_string())\n",
				"\n",
				"print(\"\\nPerformance by Category (Bottom 10):\")\n",
				"print(category_performance.tail(10).to_string())"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Visualize category performance\n",
				"fig, ax = plt.subplots(figsize=(12, 8))\n",
				"\n",
				"# Get top 15 and bottom 15 categories by F1\n",
				"top_cats = category_performance.head(10)\n",
				"bottom_cats = category_performance.tail(10)\n",
				"\n",
				"plot_data = pd.concat([top_cats, bottom_cats]).sort_values('f1_correct')\n",
				"\n",
				"colors = ['#e74c3c' if x < 0.3 else '#f39c12' if x < 0.5 else '#2ecc71' \n",
				"          for x in plot_data['f1_correct']]\n",
				"\n",
				"bars = ax.barh(range(len(plot_data)), plot_data['f1_correct'], color=colors)\n",
				"ax.set_yticks(range(len(plot_data)))\n",
				"ax.set_yticklabels(plot_data.index)\n",
				"ax.set_xlabel('Mean F1 Score')\n",
				"ax.set_title('TruthfulQA Baseline:  Performance by Category\\n(Top 10 and Bottom 10)')\n",
				"ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, label='50% threshold')\n",
				"\n",
				"# Add value labels\n",
				"for bar, val in zip(bars, plot_data['f1_correct']):\n",
				"    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.2f}', va='center', fontsize=9)\n",
				"\n",
				"plt.tight_layout()\n",
				"plt.savefig('../paper/figures/truthfulqa_baseline_by_category.png', dpi=300, bbox_inches='tight')\n",
				"plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 2. HotpotQA Baseline Experiments\n",
				"\n",
				"Now let's establish baseline on HotpotQA for multi-hop reasoning."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Load HotpotQA (limited sample for cost management)\n",
				"HOTPOTQA_SAMPLE_SIZE = 50\n",
				"\n",
				"hotpotqa = HotpotQADataset('../data/raw/hotpot_dev_distractor_v1.json', max_examples=500)\n",
				"print(f\"Loaded HotpotQA: {len(hotpotqa)} questions\")\n",
				"\n",
				"# Get statistics\n",
				"stats = hotpotqa.get_statistics()\n",
				"print(f\"Question types: {stats.get('question_types', {})}\")\n",
				"print(f\"Difficulty levels: {stats.get('difficulty_levels', {})}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Sample HotpotQA questions - balance by difficulty\n",
				"hotpotqa_samples = []\n",
				"\n",
				"for difficulty in ['easy', 'medium', 'hard']:\n",
				"    diff_examples = hotpotqa.get_by_difficulty(difficulty)\n",
				"    n_samples = min(HOTPOTQA_SAMPLE_SIZE // 3, len(diff_examples))\n",
				"    random.seed(RANDOM_SEED)\n",
				"    hotpotqa_samples.extend(random.sample(diff_examples, n_samples))\n",
				"\n",
				"random.shuffle(hotpotqa_samples)\n",
				"print(f\"Selected {len(hotpotqa_samples)} HotpotQA samples\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def run_hotpotqa_experiment(\n",
				"    samples: list,\n",
				"    llm: GeminiClient,\n",
				"    prompt_template: str = \"baseline_qa_with_context\",\n",
				"    experiment_name: str = \"baseline\"\n",
				") -> tuple[MetricsCalculator, list[dict]]:\n",
				"    \"\"\"Run HotpotQA experiment with given prompt template.\"\"\"\n",
				"    calculator = MetricsCalculator()\n",
				"    raw_results = []\n",
				"    prompt = get_prompt(prompt_template)\n",
				"    \n",
				"    print(f\"Running experiment: {experiment_name}\")\n",
				"    print(f\"Prompt template: {prompt_template}\")\n",
				"    print(f\"Samples: {len(samples)}\")\n",
				"    print(\"-\" * 50)\n",
				"    \n",
				"    for example in tqdm(samples, desc=experiment_name):\n",
				"        # Format prompt with context\n",
				"        formatted_prompt = prompt.format(\n",
				"            question=example.question,\n",
				"            context=example.context[: 4000] if example.context else \"\"  # Limit context length\n",
				"        )\n",
				"        \n",
				"        # Get response\n",
				"        try:\n",
				"            response = llm.generate(formatted_prompt, max_tokens=200)\n",
				"            response_text = response.text.strip()\n",
				"            error = None\n",
				"        except Exception as e:\n",
				"            response_text = \"\"\n",
				"            error = str(e)\n",
				"        \n",
				"        # Calculate metrics\n",
				"        result = calculator.add_result(\n",
				"            example_id=example.id,\n",
				"            prediction=response_text,\n",
				"            ground_truth=example.correct_answer,\n",
				"            metadata={\n",
				"                \"question_type\": example.category,\n",
				"                \"difficulty\": example.difficulty,\n",
				"                \"experiment\": experiment_name,\n",
				"            }\n",
				"        )\n",
				"        \n",
				"        # Store raw result\n",
				"        raw_results.append({\n",
				"            \"id\": example.id,\n",
				"            \"question\": example.question,\n",
				"            \"correct_answer\": example.correct_answer,\n",
				"            \"model_response\": response_text,\n",
				"            \"question_type\": example.category,\n",
				"            \"difficulty\": example.difficulty,\n",
				"            \"f1_score\": result[\"f1_correct\"],\n",
				"            \"exact_match\": result[\"exact_match_correct\"],\n",
				"            \"error\": error,\n",
				"        })\n",
				"    \n",
				"    return calculator, raw_results"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Run baseline experiment on HotpotQA\n",
				"hotpot_baseline_calc, hotpot_baseline_results = run_hotpotqa_experiment(\n",
				"    samples=hotpotqa_samples,\n",
				"    llm=llm,\n",
				"    prompt_template=\"baseline_qa_with_context\",\n",
				"    experiment_name=\"hotpotqa_baseline\"\n",
				")\n",
				"\n",
				"print(\"\\n\" + \"=\" * 50)\n",
				"print(\"BASELINE RESULTS (HotpotQA)\")\n",
				"print(\"=\" * 50)\n",
				"hotpot_baseline_metrics = hotpot_baseline_calc.get_aggregate_metrics()\n",
				"for key, value in hotpot_baseline_metrics.items():\n",
				"    if isinstance(value, float):\n",
				"        print(f\"{key}: {value:.4f}\")\n",
				"    else:\n",
				"        print(f\"{key}: {value}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Run chain-of-thought experiment on HotpotQA\n",
				"hotpot_cot_calc, hotpot_cot_results = run_hotpotqa_experiment(\n",
				"    samples=hotpotqa_samples,\n",
				"    llm=llm,\n",
				"    prompt_template=\"cot_multi_hop\",\n",
				"    experiment_name=\"hotpotqa_cot\"\n",
				")\n",
				"\n",
				"print(\"\\n\" + \"=\" * 50)\n",
				"print(\"CHAIN-OF-THOUGHT RESULTS (HotpotQA)\")\n",
				"print(\"=\" * 50)\n",
				"hotpot_cot_metrics = hotpot_cot_calc.get_aggregate_metrics()\n",
				"for key, value in hotpot_cot_metrics.items():\n",
				"    if isinstance(value, float):\n",
				"        print(f\"{key}: {value:.4f}\")\n",
				"    else:\n",
				"        print(f\"{key}:  {value}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Analyze HotpotQA results by difficulty\n",
				"hotpot_df = pd.DataFrame(hotpot_baseline_results)\n",
				"\n",
				"difficulty_performance = hotpot_df.groupby('difficulty').agg({\n",
				"    'f1_score': 'mean',\n",
				"    'exact_match': 'mean',\n",
				"    'id': 'count'\n",
				"}).rename(columns={'id': 'count'})\n",
				"\n",
				"# Reorder\n",
				"difficulty_order = ['easy', 'medium', 'hard']\n",
				"difficulty_performance = difficulty_performance.reindex(difficulty_order)\n",
				"\n",
				"print(\"HotpotQA Performance by Difficulty:\")\n",
				"print(difficulty_performance.to_string())"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Analyze by question type (bridge vs comparison)\n",
				"type_performance = hotpot_df.groupby('question_type').agg({\n",
				"    'f1_score':  'mean',\n",
				"    'exact_match':  'mean',\n",
				"    'id': 'count'\n",
				"}).rename(columns={'id': 'count'})\n",
				"\n",
				"print(\"\\nHotpotQA Performance by Question Type:\")\n",
				"print(type_performance.to_string())"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Visualize HotpotQA results\n",
				"fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
				"\n",
				"# By difficulty\n",
				"colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
				"axes[0].bar(difficulty_performance.index, difficulty_performance['f1_score'], color=colors)\n",
				"axes[0].set_xlabel('Difficulty')\n",
				"axes[0].set_ylabel('Mean F1 Score')\n",
				"axes[0].set_title('HotpotQA: Performance by Difficulty')\n",
				"axes[0].set_ylim(0, 1)\n",
				"for i, (idx, row) in enumerate(difficulty_performance.iterrows()):\n",
				"    axes[0].text(i, row['f1_score'] + 0.02, f\"{row['f1_score']:.2f}\", ha='center')\n",
				"\n",
				"# By question type\n",
				"colors2 = ['#3498db', '#9b59b6']\n",
				"axes[1].bar(type_performance.index, type_performance['f1_score'], color=colors2)\n",
				"axes[1].set_xlabel('Question Type')\n",
				"axes[1].set_ylabel('Mean F1 Score')\n",
				"axes[1].set_title('HotpotQA: Performance by Question Type')\n",
				"axes[1].set_ylim(0, 1)\n",
				"for i, (idx, row) in enumerate(type_performance.iterrows()):\n",
				"    axes[1].text(i, row['f1_score'] + 0.02, f\"{row['f1_score']:.2f}\", ha='center')\n",
				"\n",
				"plt.tight_layout()\n",
				"plt.savefig('../paper/figures/hotpotqa_baseline_analysis.png', dpi=300, bbox_inches='tight')\n",
				"plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 3. Save Results"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Save all results\n",
				"timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
				"\n",
				"# TruthfulQA results\n",
				"truthfulqa_output = {\n",
				"    \"experiment\": \"truthfulqa_baseline\",\n",
				"    \"timestamp\": timestamp,\n",
				"    \"model\": llm.model_name,\n",
				"    \"sample_size\": len(truthfulqa_samples),\n",
				"    \"metrics\": {\n",
				"        \"baseline\": baseline_metrics,\n",
				"        \"chain_of_thought\": cot_metrics,\n",
				"    },\n",
				"    \"results\": {\n",
				"        \"baseline\": baseline_results,\n",
				"        \"chain_of_thought\": cot_results,\n",
				"    }\n",
				"}\n",
				"\n",
				"with open(RESULTS_DIR / f\"truthfulqa_baseline_{timestamp}.json\", \"w\") as f:\n",
				"    json.dump(truthfulqa_output, f, indent=2, default=str)\n",
				"\n",
				"# HotpotQA results\n",
				"hotpotqa_output = {\n",
				"    \"experiment\": \"hotpotqa_baseline\",\n",
				"    \"timestamp\": timestamp,\n",
				"    \"model\": llm.model_name,\n",
				"    \"sample_size\": len(hotpotqa_samples),\n",
				"    \"metrics\": {\n",
				"        \"baseline\": hotpot_baseline_metrics,\n",
				"        \"chain_of_thought\": hotpot_cot_metrics,\n",
				"    },\n",
				"    \"results\": {\n",
				"        \"baseline\": hotpot_baseline_results,\n",
				"        \"chain_of_thought\": hotpot_cot_results,\n",
				"    }\n",
				"}\n",
				"\n",
				"with open(RESULTS_DIR / f\"hotpotqa_baseline_{timestamp}.json\", \"w\") as f:\n",
				"    json.dump(hotpotqa_output, f, indent=2, default=str)\n",
				"\n",
				"print(f\"Results saved to {RESULTS_DIR}\")\n",
				"print(f\"  - truthfulqa_baseline_{timestamp}.json\")\n",
				"print(f\"  - hotpotqa_baseline_{timestamp}.json\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 4. Summary\n",
				"\n",
				"### Key Findings"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"print(\"=\" * 70)\n",
				"print(\"BASELINE EXPERIMENTS SUMMARY\")\n",
				"print(\"=\" * 70)\n",
				"\n",
				"print(f\"\\nModel: {llm.model_name}\")\n",
				"print(f\"API:  {llm.get_api_type()}\")\n",
				"\n",
				"print(\"\\n\" + \"-\" * 70)\n",
				"print(\"TRUTHFULQA RESULTS\")\n",
				"print(\"-\" * 70)\n",
				"print(f\"Sample size: {len(truthfulqa_samples)} questions\")\n",
				"print(f\"\\nBaseline prompting: \")\n",
				"print(f\"  - Mean F1 (correct): {baseline_metrics.get('mean_f1_correct', 0):.4f}\")\n",
				"print(f\"  - Mean Truthfulness: {baseline_metrics.get('mean_truthfulness', 0):.4f}\")\n",
				"print(f\"\\nChain-of-Thought prompting:\")\n",
				"print(f\"  - Mean F1 (correct): {cot_metrics.get('mean_f1_correct', 0):.4f}\")\n",
				"print(f\"  - Mean Truthfulness:  {cot_metrics.get('mean_truthfulness', 0):.4f}\")\n",
				"\n",
				"print(\"\\n\" + \"-\" * 70)\n",
				"print(\"HOTPOTQA RESULTS\")\n",
				"print(\"-\" * 70)\n",
				"print(f\"Sample size: {len(hotpotqa_samples)} questions\")\n",
				"print(f\"\\nBaseline prompting:\")\n",
				"print(f\"  - Mean F1: {hotpot_baseline_metrics.get('mean_f1_correct', 0):.4f}\")\n",
				"print(f\"  - Exact Match: {hotpot_baseline_metrics.get('mean_exact_match_correct', 0):.4f}\")\n",
				"print(f\"\\nChain-of-Thought prompting:\")\n",
				"print(f\"  - Mean F1: {hotpot_cot_metrics.get('mean_f1_correct', 0):.4f}\")\n",
				"print(f\"  - Exact Match: {hotpot_cot_metrics.get('mean_exact_match_correct', 0):.4f}\")\n",
				"\n",
				"print(\"\\n\" + \"=\" * 70)\n",
				"print(\"These baselines will be compared against perturbed experiments.\")\n",
				"print(\"Next:  Run 04_perturbation_experiments.ipynb\")\n",
				"print(\"=\" * 70)"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"name": "python",
			"version": "3.10.0"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}